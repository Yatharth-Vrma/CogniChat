-- Enable the pgvector extension
-- This is a one-time setup for your project.
create extension if not exists vector;

-- Create a table to store your document chunks and their embeddings
-- The 'embedding' column is of type 'vector(768)', where 768 is the dimension
-- of the embeddings generated by Google's "embedding-001" model.
create table if not exists documents (
  id bigserial primary key,
  content text not null,
  embedding vector(768),
  created_at timestamptz default now()
);

-- Create a function to search for similar documents
-- This function takes a query embedding and search parameters,
-- and returns the most similar document chunks.
create or replace function match_documents (
  query_embedding vector(768),
  match_threshold float,
  match_count int
)
returns table (
  id bigint,
  content text,
  similarity float
)
language sql stable
as $$
  select
    documents.id,
    documents.content,
    1 - (documents.embedding <=> query_embedding) as similarity
  from documents
  where 1 - (documents.embedding <=> query_embedding) > match_threshold
  order by similarity desc
  limit match_count;
$$;

-- Create an index to speed up similarity searches
-- This is crucial for performance on large datasets.
-- The 'ivfflat' index is a good choice for this use case.
-- The 'lists' parameter is a trade-off between build time and search speed.
-- A good starting point is lists = (number of rows) / 1000 for up to 1M rows.
-- For larger datasets, consider lists = sqrt(number of rows).
create index if not exists on documents using ivfflat (embedding vector_cosine_ops)
with (lists = 100);
